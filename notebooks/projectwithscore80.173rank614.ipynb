{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNB0DPHFr1QtuwkfjeV9mZ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NwQuyCFnC289"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder, RobustScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import accuracy_score\n","from xgboost import XGBClassifier\n"]},{"cell_type":"code","source":["# Load data\n","train_df = pd.read_csv(\"/content/train.csv\")\n","test_df = pd.read_csv(\"/content/test.csv\")\n"],"metadata":{"id":"_u1v9p7hDFqZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Basic Cleaning ===\n","\n","# 1. Fill numeric columns with median\n","num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n","train_df[num_cols] = train_df[num_cols].fillna(train_df[num_cols].median())\n","test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].median())\n","\n","# 2. Fill categorical columns with mode\n","cat_cols = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']\n","for col in cat_cols:\n","    if col in train_df.columns:\n","        train_df[col] = train_df[col].fillna(train_df[col].mode()[0])\n","        train_df[col] = train_df[col].infer_objects(copy=False)\n","    if col in test_df.columns:\n","        test_df[col] = test_df[col].fillna(test_df[col].mode()[0])\n","        test_df[col] = test_df[col].infer_objects(copy=False)\n","        # Load original data for passenger IDs\n","original_train = pd.read_csv('/content/train.csv')\n","original_test = pd.read_csv('/content/test.csv')\n","\n","train_ids = original_train['PassengerId']\n","test_ids = original_test['PassengerId']\n","\n","train_df['Group'] = train_ids.str.split('_').str[0]\n","test_df['Group'] = test_ids.str.split('_').str[0]\n","\n","train_df['GroupSize'] = train_df.groupby('Group')['Group'].transform('count')\n","test_df['GroupSize'] = test_df.groupby('Group')['Group'].transform('count')\n","\n","train_df['IsSolo'] = (train_df['GroupSize'] == 1).astype(int)\n","test_df['IsSolo'] = (test_df['GroupSize'] == 1).astype(int)\n","\n","train_df.drop(columns='Group', inplace=True)\n","test_df.drop(columns='Group', inplace=True)\n","\n","# Now drop PassengerId and Name safely\n","train_df.drop(columns=['PassengerId', 'Name'], inplace=True)\n","test_df.drop(columns=['PassengerId', 'Name'], inplace=True)\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w5guGfmXDIyu","executionInfo":{"status":"ok","timestamp":1753863610080,"user_tz":-180,"elapsed":154,"user":{"displayName":"Salma Azoz","userId":"17533175569660180680"}},"outputId":"6fea2b0e-d1e2-4e92-ec5c-3372d21f56d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3-1964411351.py:12: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  train_df[col] = train_df[col].fillna(train_df[col].mode()[0])\n","/tmp/ipython-input-3-1964411351.py:15: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  test_df[col] = test_df[col].fillna(test_df[col].mode()[0])\n"]}]},{"cell_type":"code","source":["# Outlier capping function (winsorizing)\n","def cap_outliers(df, cols, lower_quantile=0.01, upper_quantile=0.99):\n","    df = df.copy()\n","    for col in cols:\n","        lower = df[col].quantile(lower_quantile)\n","        upper = df[col].quantile(upper_quantile)\n","        df[col] = df[col].clip(lower, upper)\n","    return df"],"metadata":{"id":"-Q8rHbLrcQ8Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feature engineering function\n","def feature_engineering(df):\n","    df = df.copy()\n","\n","    # Extract group info\n","    df['Group'] = df['PassengerId'].str.split('_').str[0]\n","    df['GroupSize'] = df.groupby('Group')['PassengerId'].transform('count')\n","    df['IsSolo'] = (df['GroupSize'] == 1).astype(int)\n","\n","    # Cabin split\n","    cabin_split = df['Cabin'].str.split('/', expand=True)\n","    df['Deck'] = cabin_split[0]\n","    df['CabinNum'] = pd.to_numeric(cabin_split[1], errors='coerce')\n","    df['Side'] = cabin_split[2]\n","\n","    # Spending columns\n","    spend_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n","    df['TotalSpend'] = df[spend_cols].sum(axis=1)\n","    df['SpendPerGroupMember'] = df['TotalSpend'] / df['GroupSize']\n","\n","    # Flag no spending\n","    df['NoSpend'] = (df[spend_cols].fillna(0).sum(axis=1) == 0).astype(int)\n","\n","    # Age groups\n","    df['AgeGroup'] = pd.cut(\n","        df['Age'],\n","        bins=[-1, 12, 18, 25, 40, 60, 120],\n","        labels=['child', 'teen', 'student', 'young_adult', 'adult', 'senior']\n","    )\n","\n","    # Age related flags\n","    df['IsMinor'] = (df['Age'] < 18).astype(int)\n","    df['IsSenior'] = (df['Age'] > 60).astype(int)\n","\n","    # Name length\n","    df['NameLength'] = df['Name'].fillna('').apply(len)\n","\n","    # Luxury deck flag\n","    luxury_decks = ['A', 'B', 'T']\n","    df['IsLuxuryDeck'] = df['Deck'].isin(luxury_decks).astype(int)\n","\n","    # Interaction feature\n","    df['AgeSpend'] = df['Age'] * df['TotalSpend']\n","\n","    # CabinNum missing flag\n","    df['CabinNumMissing'] = df['CabinNum'].isna().astype(int)\n","\n","    # Drop unused columns\n","    df = df.drop(columns=['PassengerId', 'Name', 'Group', 'Cabin'])\n","\n","    return df\n","\n","# Apply feature engineering\n","train_fe = feature_engineering(train_df)\n","test_fe = feature_engineering(test_df)\n","\n","# List of numeric columns to cap outliers on\n","num_outlier_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n","                    'TotalSpend', 'SpendPerGroupMember', 'AgeSpend', 'CabinNum', 'NameLength']\n","\n","# Cap outliers in train set only (don't modify test target distribution)\n","train_fe = cap_outliers(train_fe, num_outlier_cols)"],"metadata":{"id":"XUshgG1FDO9Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separate target and features\n","y = train_fe['Transported'].astype(int)\n","X = train_fe.drop(columns='Transported')\n","\n"],"metadata":{"id":"m1bgTYAeDu_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define numeric and categorical columns\n","num_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n","            'TotalSpend', 'SpendPerGroupMember', 'NameLength', 'AgeSpend', 'CabinNum']\n","\n","cat_cols = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side', 'AgeGroup',\n","            'IsSolo', 'IsMinor', 'IsSenior', 'IsLuxuryDeck', 'NoSpend', 'CabinNumMissing']\n","\n","# Pipelines for preprocessing\n","numeric_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler', RobustScaler())\n","])\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='most_frequent')),\n","    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe, num_cols),\n","    ('cat', categorical_pipe, cat_cols)\n","])\n"],"metadata":{"id":"kCrVWSeCDxX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model\n","xgb = XGBClassifier(\n","    objective='binary:logistic',\n","    eval_metric='logloss',\n","    use_label_encoder=False,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# Full pipeline\n","pipe = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('classifier', xgb)\n","])\n","\n","# Hyperparameter grid for Randomized Search\n","param_dist = {\n","    'classifier__n_estimators': [100, 200, 300],\n","    'classifier__max_depth': [3, 4, 5, 6],\n","    'classifier__learning_rate': [0.01, 0.03, 0.05, 0.1],\n","    'classifier__subsample': [0.6, 0.7, 0.8, 1.0],\n","    'classifier__colsample_bytree': [0.6, 0.7, 0.8, 1.0],\n","    'classifier__min_child_weight': [1, 3, 5]\n","}\n","\n","# Stratified K-Fold CV\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# Initialize RandomizedSearchCV without early stopping params\n","search = RandomizedSearchCV(\n","    pipe,\n","    param_distributions=param_dist,\n","    n_iter=20,\n","    scoring='accuracy',\n","    n_jobs=-1,\n","    cv=cv,\n","    verbose=2,\n","    random_state=42,\n","    refit=True\n",")\n","\n","# Run hyperparameter tuning without early stopping parameters\n","search.fit(X, y)\n","\n","print(f\"Best CV accuracy: {search.best_score_:.5f}\")\n","print(\"Best hyperparameters:\", search.best_params_)\n","\n","# Best model retrained on full data\n","best_model = search.best_estimator_\n","\n","# Predict on test data\n","test_preds = best_model.predict(test_fe)\n","\n","# Prepare submission file\n","submission = pd.DataFrame({\n","    'PassengerId': test_df['PassengerId'],\n","    'Transported': test_preds.astype(bool)\n","})\n","\n","submission.to_csv('/content/submission.csv', index=False)\n","print(\"Submission saved to /content/submission.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Ys1N0FvdKSO","executionInfo":{"status":"ok","timestamp":1753864426963,"user_tz":-180,"elapsed":48515,"user":{"displayName":"Salma Azoz","userId":"17533175569660180680"}},"outputId":"d73d6c13-ed95-4be8-dd6b-3cc7ecb61e07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [08:33:47] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["Best CV accuracy: 0.80950\n","Best hyperparameters: {'classifier__subsample': 1.0, 'classifier__n_estimators': 100, 'classifier__min_child_weight': 3, 'classifier__max_depth': 5, 'classifier__learning_rate': 0.1, 'classifier__colsample_bytree': 0.7}\n","Submission saved to /content/submission.csv\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.model_selection import train_test_split\n","\n","\n","X_train_dl, X_val_dl, y_train_dl, y_val_dl = train_test_split(\n","    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# Build a simple MLP model\n","model = models.Sequential([\n","    layers.Input(shape=(X_scaled.shape[1],)),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dropout(0.3),\n","    layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(\n","    optimizer='adam',\n","    loss='binary_crossentropy',\n","    metrics=['accuracy']\n",")\n","\n","# Train model with early stopping\n","early_stop = tf.keras.callbacks.EarlyStopping(\n","    monitor='val_loss', patience=5, restore_best_weights=True\n",")\n","\n","history = model.fit(\n","    X_train_dl, y_train_dl,\n","    validation_data=(X_val_dl, y_val_dl),\n","    epochs=50,\n","    batch_size=32,\n","    callbacks=[early_stop],\n","    verbose=2\n",")\n","\n","# Evaluate on validation set\n","val_loss, val_acc = model.evaluate(X_val_dl, y_val_dl, verbose=0)\n","print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n","\n","\n"],"metadata":{"id":"HoB_j_lM5-rP"},"execution_count":null,"outputs":[]}]}